{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zijieguo/anaconda2/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='elementwise_mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "cfg_file = 'config/v390.cfg'\n",
    "weights_file = 'weights/v390_280000.weights'\n",
    "video_file = '/home/zijieguo/project/darknet/IMG_8765.mov'\n",
    "\n",
    "\n",
    "model = Darknet(cfg_file, img_size=416)\n",
    "model.load_weights(weights_file)\n",
    "\n",
    "input_dim = 416\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "cap = cv2.VideoCapture(video_file)\n",
    "\n",
    "assert cap.isOpened(), 'failed to load camera video'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 0\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame= cap.read()\n",
    "if ret:\n",
    "    img = prep_image(frame, input_dim).cuda()\n",
    "    img_dim = frame.shape[1], frame.shape[0]\n",
    "    img_dim = torch.FloatTensor(img_dim).repeat(1,2).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        detections = model(img)\n",
    "#         detections = write_results(detections, 0.8, 80, 0.4)\n",
    "        \n",
    "#     if type(detections) == int:\n",
    "#         frames += 1\n",
    "#         print(\"FPS of the video is {:5.4f}\".format( frames / (time.time() - start_time)))\n",
    "#         cv2.imshow(\"frame\", frame)\n",
    "#         key = cv2.waitKey(1)\n",
    "#         if key & 0xFF == ord('q'):\n",
    "#             break\n",
    "#         continue\n",
    "\n",
    "#     print(detections)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Tensors must have same number of dimensions: got 1 and 2 at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THC/generic/THCTensorMath.cu:78",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-0248f7abf71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmax_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_conf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_conf_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimage_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mimage_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Tensors must have same number of dimensions: got 1 and 2 at /opt/conda/conda-bld/pytorch_1532579245307/work/aten/src/THC/generic/THCTensorMath.cu:78"
     ]
    }
   ],
   "source": [
    "box_corner = detections.new(detections.shape)\n",
    "# prediction[:,:,0] center x, 1 center y , 2 box_width, 3 box_height\n",
    "box_corner[:,:,0] = (detections[:,:,0] - detections[:,:,2]/2)\n",
    "box_corner[:,:,1] = (detections[:,:,1] - detections[:,:,3]/2)\n",
    "box_corner[:,:,2] = (detections[:,:,0] + detections[:,:,2]/2) \n",
    "box_corner[:,:,3] = (detections[:,:,1] + detections[:,:,3]/2)\n",
    "detections[:,:,:4] = box_corner[:,:,:4]\n",
    "# prediction is using x,y,w,h to describe the boundingbox, convert to the 4 corner point\n",
    "batch_size = detections.size(0)\n",
    "write = False\n",
    "for index in range(batch_size):\n",
    "    image_pred = detections[index]\n",
    "    max_conf, max_conf_score = torch.max(image_pred[:,5:85],1)\n",
    "    max_conf = max_conf.float().unsqueeze(1)\n",
    "    seq = (image_pred[:,:5], max_conf, max_conf_score.float())\n",
    "    image_pred = torch.cat(seq,1)\n",
    "    image_pred.size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 5])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pred[:,:5].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10647])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_conf_score.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.2450, 2.8629]), tensor([1, 2]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(a[:,2:4], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5431,  0.6298,  1.3207],\n",
       "        [-0.1956,  0.1025, -0.9766]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 416, 416])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " input_dim).cuda()\n",
    "img.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
