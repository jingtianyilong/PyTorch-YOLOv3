{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting time:  1556627764.2647052\n",
      "\n",
      "Performing object detection:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[  5.0433, 208.0689, 133.5634, 274.0513,   0.9992,   0.9995,   2.0000],\n",
       "         [202.3678, 203.0705, 242.1279, 233.4327,   0.9980,   0.9999,   2.0000],\n",
       "         [118.8502, 203.6419, 208.2199, 263.8680,   0.9891,   0.9758,   2.0000],\n",
       "         [271.3281, 201.2100, 280.0563, 207.2144,   0.9638,   0.9832,   2.0000],\n",
       "         [314.0755, 218.4319, 416.3620, 269.0359,   0.9204,   0.9999,   2.0000],\n",
       "         [293.7480, 207.2141, 319.6446, 226.7405,   0.8638,   0.9870,   2.0000],\n",
       "         [248.1488, 202.3865, 267.4640, 215.2374,   0.8588,   0.9984,   2.0000]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from __future__ import division\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "from utils.kittiloader import *\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "import time\n",
    "\n",
    "\n",
    "image_folder = 'examples/'\n",
    "config_path = 'config/v390.cfg'\n",
    "weights_path = 'weights/v390_final.weights'\n",
    "kitti_path = '/home/project/ZijieMA/KITTI/'\n",
    "class_path = 'data/coco.names'\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "batch_size = 1\n",
    "n_cpu = 16\n",
    "img_size = 416\n",
    "use_cuda = True\n",
    "CUDA_available = torch.cuda.is_available() and use_cuda\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "model = Darknet(config_path, img_size=img_size)\n",
    "model.load_weights(weights_path)\n",
    "input_dim = img_size\n",
    "\n",
    "if CUDA_available:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# dataloader = DataLoader(ImageFolder( opt.kitti_path + 'testing/image_2', img_size=opt.img_size),\n",
    "                        # batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n",
    "dataloader = DataLoader(ImageFolder(image_folder, img_size=img_size),\n",
    "                        batch_size=batch_size, shuffle=False, num_workers=n_cpu)\n",
    "\n",
    "classes = load_classes(class_path)\n",
    "# TODO: different class file for \n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if CUDA_available else torch.FloatTensor\n",
    "\n",
    "\n",
    "imgs = []           # Stores image paths\n",
    "img_detections = [] # Stores detections for each image index\n",
    "start_time = time.time()\n",
    "print('starting time: ', start_time)\n",
    "print ('\\nPerforming object detection:')\n",
    "prev_time = time.time()\n",
    "inference_time = datetime.timedelta(seconds=prev_time - prev_time)\n",
    "for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "\n",
    "    # Configure input\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "    # Viriable API has been deprecated. Viriable(tensor) return Tensors instead of Variable\n",
    "    # transform input_imgs to Tensor\n",
    "\n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs) # size 1x10647x85\n",
    "        detections = non_max_suppression(detections, 80, conf_thres, nms_thres)\n",
    "        # 有一些任务，可能事先需要设置，事后做清理工作。对于这种场景，Python的with语句提供了一种非常方便的处理方式。\n",
    "        # 一个很好的例子是文件处理，你需要获取一个文件句柄，从文件中读取数据，然后关闭文件句柄。基本思想是with所求值\n",
    "        # 的对象必须有一个__enter__()方法，一个__exit__()方法。紧跟with后面的语句被求值后，返回对象的__enter__()\n",
    "        # 方法被调用，这个方法的返回值将被赋值给as后面的变量。当with后面的代码块全部被执行完之后，将调用前面返回对象的\n",
    "        # __exit__()方法。\n",
    "        # similar to: try: handle = open(file) ; ...; finally: handle.close()\n",
    "\n",
    "    # detections = torch.cat(detections)\n",
    "\n",
    "    #  TODO: img_index -> lidar_index -> lidar filter.\n",
    "    # add one column to for distance of the object\n",
    "    # detections_with_distance = np.zeros((detections[0].shape[0],detections[0].shape[1]+1))\n",
    "    if detections[0] is not None:\n",
    "        detections_with_distance = torch.zeros((detections[0].shape[0],detections[0].shape[1]+1))\n",
    "        detections_with_distance[:,:-1] = detections[0]\n",
    "detections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-965d529eb199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "detection = detections.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.019116776808165\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-299791b1f931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# remove points that are located behind the camera:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mpoint_cloud\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoint_cloud\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mD_rough\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoint_cloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# remove points that are located too far away from the camera:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "for detection in detections_with_distance:\n",
    "    img_id = 8\n",
    "    img_path = 'examples/000008.png'\n",
    "    detection = detection.numpy()\n",
    "    img_size_after_resize = img_size\n",
    "    lidar_path = '%straining/velodyne/%06d.bin' % (kitti_path, img_id)\n",
    "    calib = calibread('%straining/calib/%06d.txt' % (kitti_path, img_id))\n",
    "    img = cv2.imread('/home/project/ZijieMA/PyTorch-YOLOv3/examples/%06d.png' % img_id, cv2.IMREAD_UNCHANGED)\n",
    "    # img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    img_width_orig = img.shape[1]\n",
    "\n",
    "    img_height_orig = img.shape[0]\n",
    "\n",
    "    pad_x = max(img_height_orig - img_width_orig, 0) * (img_size_after_resize / max(img_width_orig, img_height_orig))\n",
    "    pad_y = max(img_width_orig - img_height_orig, 0) * (img_size_after_resize / max(img_width_orig, img_height_orig))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = img_size_after_resize - pad_y\n",
    "    unpad_w = img_size_after_resize - pad_x\n",
    "    box_h = ((detection[3] - detection[1]) / unpad_h) * img_height_orig\n",
    "    box_w = ((detection[2] - detection[0]) / unpad_w) * img_width_orig\n",
    "    v_upper = ((detection[1] - pad_y // 2) / unpad_h) * img_height_orig\n",
    "    u_left = ((detection[0] - pad_x // 2) / unpad_w) * img_width_orig\n",
    "    v_bottom = v_upper + box_h\n",
    "    u_right = u_left + box_w\n",
    "\n",
    "    point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n",
    "    # orig_point_cloud = point_cloud # nx4\n",
    "\n",
    "    # detections with shape: (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "\n",
    "    ########################################################################\n",
    "    # Distance rough estimation\n",
    "    # D = H [tan(theta_c+arctan((h_i/2-d_p)/(h/(2*tan(alpha/2)))))-tan(theta_c-alpha/2)]\n",
    "    # reference：\n",
    "    # Computer_Vision_for_Road_Safety_A_System\n",
    "    # H         height of the camera(according to kitti is 1.65m)\n",
    "    # alpha     angle of FOV in v-axis fv=h_i/(2*tan(alpha/2))\n",
    "    # theta_c   angle between camera x-axis and X-axis(pi/2)\n",
    "    # h_i       height of the recorded image plane(pixel)(512)\n",
    "    # d_p       distance from the bottom of image to the bottom of the bounding box(512-v_min)\n",
    "    #\n",
    "    # D = H ×[tan（theta_c+arctan((h_i-d_p)/fv))-tan(theta_c-arctan(hi/(2fv)))]\n",
    "    #\n",
    "    ########################################################################\n",
    "\n",
    "\n",
    "    # # # # # debug visualization:\n",
    "    # pcd = PointCloud()\n",
    "    # pcd.points = Vector3dVector(point_cloud[:, 0:3])\n",
    "    # pcd.paint_uniform_color([0.65, 0.65, 0.65])\n",
    "    # draw_geometries_dark_background([pcd])\n",
    "    # 362 252 207 214\n",
    "    # # # # #\n",
    "\n",
    "    P2 = calib[\"P2\"] # 3x4 matris projection matrix after rectification\n",
    "    # （u,v,1） = dot(P2, (x,y,z,1))\n",
    "    Height_of_camera = 1.65\n",
    "    fu = P2[0][0]  # for horizontal position\n",
    "    fv = P2[1][1]\n",
    "    # theta_c = np.pi/2\n",
    "    D_rough = Height_of_camera * fv / (v_bottom - img_height_orig/2)\n",
    "    # D_rough = Height_of_camera * (np.tan(theta_c + np.arctan((img_height_orig/2 - d_p)/fv)) - np.tan(theta_c - np.arctan(img_height_orig/(2*fv))))\n",
    "    print(D_rough)\n",
    "    if D_rough > 0:\n",
    "        # remove points that are located behind the camera:\n",
    "        point_cloud = point_cloud[point_cloud[:, 0] > (D_rough - 2), :]\n",
    "        print(point_cloud.shape())\n",
    "        \n",
    "        # remove points that are located too far away from the camera:\n",
    "        point_cloud = point_cloud[point_cloud[:, 0] < min(80, D_rough + 2), :]\n",
    "#         print(point_cloud.shape())\n",
    "\n",
    "        point_cloud = point_cloud[point_cloud[:,2] > Height_of_camera,:]\n",
    "#         print(point_cloud.shape())\n",
    "\n",
    "\n",
    "\n",
    "#         Tr_velo_to_cam_orig = calib[\"Tr_velo_to_cam\"]\n",
    "#         R0_rect_orig = calib[\"R0_rect\"] # 3x3\n",
    "\n",
    "#         R0_rect = np.eye(4)\n",
    "#         R0_rect[0:3, 0:3] = R0_rect_orig # 3x3 -> 4x4 up left corner\n",
    "#         ########################################################################\n",
    "#         # R0_rect: example\n",
    "#         # array([[ 0.99, 0.01, 0.01,   0 ],\n",
    "#         #        [ 0.01, 0.99, 0.01,   0 ],\n",
    "#         #        [ 0.01, 0.01, 0.99,   0 ],\n",
    "#         #        [    0,    0,    0,   1 ]])\n",
    "#         ########################################################################\n",
    "\n",
    "#         Tr_velo_to_cam = np.eye(4)\n",
    "#         Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig # 3x4 -> 4x4 up left corner\n",
    "#         ########################################################################\n",
    "#         # Tr_velo_to_cam:\n",
    "#         # Tr_velo_to_cam = [ R_velo_to_cam,    t_velo_to_cam ]\n",
    "#         #                  [             0,                1 ]\n",
    "#         # Rotation matrix velo -> camera 3x3, translation vector velo ->camera 1x3\n",
    "#         ########################################################################\n",
    "\n",
    "#         point_cloud_xyz = point_cloud[:, 0:3] # num_point x 3 (x,y,z,reflectance) reflectance don't need\n",
    "#         point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n",
    "#         point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n",
    "#         # the 4th column are all 1\n",
    "\n",
    "#         # project the points onto the image plane (homogeneous coords):\n",
    "#         img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n",
    "#         # (U,V,_) = P2 * R0_rect * Tr_velo_to_cam * point_cloud_xyz_hom\n",
    "#         # normalize: (U,V,1)\n",
    "#         img_points = np.zeros((img_points_hom.shape[0], 2))\n",
    "#         img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n",
    "#         img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n",
    "\n",
    "#         # transform the points into (rectified) camera coordinates:\n",
    "#         point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n",
    "#         # normalize: (x,y,z,1)\n",
    "#         point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n",
    "#         point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n",
    "#         point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n",
    "#         point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n",
    "\n",
    "#         point_cloud_camera = point_cloud\n",
    "#         point_cloud_camera[:, 0:3] = point_cloud_xyz_camera # reserve reflection\n",
    "\n",
    "#         ########################################################################\n",
    "#         # point_cloud               n x 4   original xyzr value before cali in velo coordinate\n",
    "#         # point_cloud_xyz           n x 3   xyz value before cali in velo coordinate\n",
    "#         # point_cloud_xyz_hom       n x 4   xyz1 in velo coordinate\n",
    "#         # point_cloud_xyz_camera    n x 4   xyz1 in camera coordinate\n",
    "#         # point_cloud_camera        n x 4   xyzr in camera coordinate\n",
    "#         # img_points_hom            n x 3   uv_\n",
    "#         # img_points                n x 2   UV\n",
    "#         ########################################################################\n",
    "\n",
    "\n",
    "#         row_mask = np.logical_and(\n",
    "#                         np.logical_and(img_points[:, 0] >= u_left,\n",
    "#                                        img_points[:, 0] <= u_right),\n",
    "#                         np.logical_and(img_points[:, 1] >= v_upper,\n",
    "#                                        img_points[:, 1] <= v_bottom))\n",
    "\n",
    "#         # filter out point are not in frustum area\n",
    "#         frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n",
    "#         frustum_point_cloud = point_cloud[row_mask, :]\n",
    "#         frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n",
    "#         frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n",
    "\n",
    "#         # randomly sample 512 points in the frustum point cloud:\n",
    "\n",
    "\n",
    "#         if frustum_point_cloud.shape[0] == 0:\n",
    "#              detection[7] = D_rough\n",
    "#              return torch.tensor(detection)\n",
    "#         # elif frustum_point_cloud.shape[0] < 512:\n",
    "#         #     row_idx = np.random.choice(frustum_point_cloud.shape[0], 512, replace=True)\n",
    "#         # else:\n",
    "#         #     row_idx = np.random.choice(frustum_point_cloud.shape[0], 512, replace=False)\n",
    "\n",
    "#         frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n",
    "#         ransac = linear_model.RANSACRegressor()\n",
    "#         ransac.fit(frustum_point_cloud_xyz_camera[:,1].reshape(-1,1),frustum_point_cloud_xyz_camera[:,0].reshape(-1,1))\n",
    "\n",
    "#         right_side_distance = ransac.predict([[frustum_point_cloud_xyz_camera[:,1].max()]])[0][0]\n",
    "#         left_side_distance = ransac.predict([[frustum_point_cloud_xyz_camera[:,1].min()]])[0][0]\n",
    "\n",
    "#         detection[7] = min(min(left_side_distance,right_side_distance),D_rough-2)\n",
    "#         print('image id:', img_id)\n",
    "#         print('Rough estimation %d, \\n ransac estimation: %d %d, \\n final estimation: %d' %(rough_D,left_side_distance,right_side_distance,detection[7]))\n",
    "#         return torch.tensor(detection)\n",
    "\n",
    "#     else:# might be a problem\n",
    "#         detection[7] = float('nan')\n",
    "#         return torch.tensor(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
