{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "from utils.kittiloader import *\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "import time\n",
    "\n",
    "\n",
    "image_folder = 'examples/'\n",
    "config_path = 'config/v390.cfg'\n",
    "weights_path = 'weights/v390_final.weights'\n",
    "kitti_path = '/home/zijieguo/project/KITTI/'\n",
    "class_path = 'data/coco.names'\n",
    "conf_thres = 0.8\n",
    "nms_thres = 0.4\n",
    "batch_size = 1\n",
    "n_cpu = 16\n",
    "img_size = 416\n",
    "use_cuda = True\n",
    "CUDA_available = torch.cuda.is_available() and use_cuda\n",
    "\n",
    "detections = torch.tensor([\n",
    "    [  5.0433, 208.0689, 133.5634, 274.0513,   0.9992,   0.9995,   2.0000],\n",
    "         [202.3678, 203.0705, 242.1279, 233.4327,   0.9980,   0.9999,   2.0000],\n",
    "         [118.8502, 203.6419, 208.2199, 263.8680,   0.9891,   0.9758,   2.0000]\n",
    "#          ,[271.3281, 201.2100, 280.0563, 207.2144,   0.9638,   0.9832,   2.0000],\n",
    "#          [314.0755, 218.4319, 416.3620, 269.0359,   0.9204,   0.9999,   2.0000],\n",
    "#          [293.7480, 207.2141, 319.6446, 226.7405,   0.8638,   0.9870,   2.0000],\n",
    "#          [248.1488, 202.3865, 267.4640, 215.2374,   0.8588,   0.9984,   2.0000]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  5.0433, 208.0689, 133.5634, 274.0513,   0.9992,   0.9995,   2.0000,\n",
       "           0.0000],\n",
       "        [202.3678, 203.0705, 242.1279, 233.4327,   0.9980,   0.9999,   2.0000,\n",
       "           0.0000],\n",
       "        [118.8502, 203.6419, 208.2199, 263.8680,   0.9891,   0.9758,   2.0000,\n",
       "           0.0000]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if detections[0] is not None:\n",
    "        detections_with_distance = torch.zeros((detections.shape[0],detections.shape[1]+1))\n",
    "        detections_with_distance[:,:-1] = detections\n",
    "detections_with_distance\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0191195494967635\n",
      "(52649, 4)\n",
      "(44653, 4)\n",
      "(34047, 4)\n",
      "15.557979818717886\n",
      "(8538, 4)\n",
      "(6167, 4)\n",
      "(5328, 4)\n",
      "7.112372944615878\n",
      "(45379, 4)\n",
      "(38000, 4)\n",
      "(27765, 4)\n"
     ]
    }
   ],
   "source": [
    "for detection in detections_with_distance:\n",
    "    img_id = 8\n",
    "    img_path = 'examples/000008.png'\n",
    "    detection = detection.numpy()\n",
    "    img_size_after_resize = img_size\n",
    "    lidar_path = '%straining/velodyne/%06d.bin' % (kitti_path, img_id)\n",
    "    calib = calibread('%straining/calib/%06d.txt' % (kitti_path, img_id))\n",
    "    img = cv2.imread('/home/zijieguo/project/PyTorch-YOLOv3/examples/%06d.png' % img_id, cv2.IMREAD_UNCHANGED)\n",
    "    # img = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "    img_width_orig = img.shape[1]\n",
    "\n",
    "    img_height_orig = img.shape[0]\n",
    "\n",
    "    pad_x = max(img_height_orig - img_width_orig, 0) * (img_size_after_resize / max(img_width_orig, img_height_orig))\n",
    "    pad_y = max(img_width_orig - img_height_orig, 0) * (img_size_after_resize / max(img_width_orig, img_height_orig))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = img_size_after_resize - pad_y\n",
    "    unpad_w = img_size_after_resize - pad_x\n",
    "    box_h = ((detection[3] - detection[1]) / unpad_h) * img_height_orig\n",
    "    box_w = ((detection[2] - detection[0]) / unpad_w) * img_width_orig\n",
    "    v_upper = ((detection[1] - pad_y // 2) / unpad_h) * img_height_orig\n",
    "    u_left = ((detection[0] - pad_x // 2) / unpad_w) * img_width_orig\n",
    "    v_bottom = v_upper + box_h\n",
    "    u_right = u_left + box_w\n",
    "\n",
    "    point_cloud = np.fromfile(lidar_path, dtype=np.float32).reshape(-1, 4)\n",
    "    # orig_point_cloud = point_cloud # nx4\n",
    "\n",
    "    # detections with shape: (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "\n",
    "\n",
    "    P2 = calib[\"P2\"] # 3x4 matris projection matrix after rectification\n",
    "    # （u,v,1） = dot(P2, (x,y,z,1))\n",
    "    Height_of_camera = 1.65\n",
    "    fu = P2[0][0]  # for horizontal position\n",
    "    fv = P2[1][1]\n",
    "    # theta_c = np.pi/2\n",
    "    D_rough = Height_of_camera * fv / (v_bottom - img_height_orig/2)\n",
    "    # D_rough = Height_of_camera * (np.tan(theta_c + np.arctan((img_height_orig/2 - d_p)/fv)) - np.tan(theta_c - np.arctan(img_height_orig/(2*fv))))\n",
    "    print(D_rough)\n",
    "    if D_rough > 0:\n",
    "        # remove points that are located behind the camera:\n",
    "        point_cloud = point_cloud[point_cloud[:, 0] > (D_rough - 5), :]\n",
    "        print(point_cloud.shape)\n",
    "        \n",
    "        # remove points that are located too far away from the camera:\n",
    "        point_cloud = point_cloud[point_cloud[:, 0] < min(80, D_rough + 5), :]\n",
    "        print(point_cloud.shape)\n",
    "\n",
    "        point_cloud = point_cloud[point_cloud[:,2] > -Height_of_camera,:]\n",
    "        print(point_cloud.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.647  3.235  0.553  0.41 ]\n",
      " [ 9.676  3.279  0.555  0.41 ]\n",
      " [ 9.654  3.305  0.554  0.66 ]\n",
      " ...\n",
      " [ 2.46   2.517 -1.525  0.04 ]\n",
      " [ 2.467  2.533 -1.533  0.17 ]\n",
      " [ 2.541  2.623 -1.587  0.14 ]]\n"
     ]
    }
   ],
   "source": [
    "print(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a120fbffab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Y and Z axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_show\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(cls, block)\u001b[0m\n\u001b[1;32m   3264\u001b[0m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3266\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3268\u001b[0m     \u001b[0;31m# This method is the one actually exporting the required methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/matplotlib/backends/_backend_tk.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0mmanagers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGcf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_fig_managers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmanagers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m             \u001b[0mmanagers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/pytorch_yolo/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1281\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "axes_limits = [\n",
    "    [-10, 80], # X axis range\n",
    "    [-30, 30], # Y axis range\n",
    "    [-3, 10]   # Z axis range\n",
    "]\n",
    "axes_str = ['X', 'Y', 'Z']\n",
    "\n",
    "def draw_point_cloud(ax, title, point_cloud, axes=[0, 1, 2], xlim3d=None, ylim3d=None, zlim3d=None):\n",
    "        \"\"\"\n",
    "        Convenient method for drawing various point cloud projections as a part of frame statistics.\n",
    "        \"\"\"\n",
    "#         ax.scatter(*np.transpose(velo_frame[:, axes]), s=point_size, c=velo_frame[:, 3], cmap='cool')\n",
    "        ax.scatter(point_cloud[:,axes[0]],point_cloud[:,axes[1]], s=1, c=point_cloud[:, 3], facecolor='black', cmap='gray')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('{} axis'.format(axes_str[axes[0]]))\n",
    "        ax.set_ylabel('{} axis'.format(axes_str[axes[1]]))\n",
    "        if len(axes) > 2:\n",
    "            ax.set_xlim3d(*axes_limits[axes[0]])\n",
    "            ax.set_ylim3d(*axes_limits[axes[1]])\n",
    "            ax.set_zlabel('{} axis'.format(axes_str[axes[2]]))\n",
    "        else:\n",
    "            ax.set_xlim(*axes_limits[axes[0]])\n",
    "            ax.set_ylim(*axes_limits[axes[1]])\n",
    "        # User specified limits\n",
    "        if xlim3d!=None:\n",
    "            ax.set_xlim3d(xlim3d)\n",
    "        if ylim3d!=None:\n",
    "            ax.set_ylim3d(ylim3d)\n",
    "        if zlim3d!=None:\n",
    "            ax.set_zlim3d(zlim3d)\n",
    "\n",
    "                       \n",
    "f, ax3 = plt.subplots(3, 1, figsize=(15, 25))\n",
    "draw_point_cloud(\n",
    "    ax3[0], \n",
    "    'Velodyne scan, XZ projection (Y = 0), the car is moving in direction left to right',\n",
    "    point_cloud,\n",
    "    axes=[0, 2] # X and Z axes\n",
    ")\n",
    "draw_point_cloud(\n",
    "    ax3[1], \n",
    "    'Velodyne scan, XY projection (Z = 0), the car is moving in direction left to right', \n",
    "    point_cloud,\n",
    "    axes=[0, 1] # X and Y axes\n",
    ")\n",
    "draw_point_cloud(\n",
    "    ax3[2], \n",
    "    'Velodyne scan, YZ projection (X = 0), the car is moving towards the graph plane', \n",
    "    point_cloud,\n",
    "    axes=[1, 2] # Y and Z axes\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#         Tr_velo_to_cam_orig = calib[\"Tr_velo_to_cam\"]\n",
    "#         R0_rect_orig = calib[\"R0_rect\"] # 3x3\n",
    "\n",
    "#         R0_rect = np.eye(4)\n",
    "#         R0_rect[0:3, 0:3] = R0_rect_orig # 3x3 -> 4x4 up left corner\n",
    "#         ########################################################################\n",
    "#         # R0_rect: example\n",
    "#         # array([[ 0.99, 0.01, 0.01,   0 ],\n",
    "#         #        [ 0.01, 0.99, 0.01,   0 ],\n",
    "#         #        [ 0.01, 0.01, 0.99,   0 ],\n",
    "#         #        [    0,    0,    0,   1 ]])\n",
    "#         ########################################################################\n",
    "\n",
    "#         Tr_velo_to_cam = np.eye(4)\n",
    "#         Tr_velo_to_cam[0:3, :] = Tr_velo_to_cam_orig # 3x4 -> 4x4 up left corner\n",
    "#         ########################################################################\n",
    "#         # Tr_velo_to_cam:\n",
    "#         # Tr_velo_to_cam = [ R_velo_to_cam,    t_velo_to_cam ]\n",
    "#         #                  [             0,                1 ]\n",
    "#         # Rotation matrix velo -> camera 3x3, translation vector velo ->camera 1x3\n",
    "#         ########################################################################\n",
    "\n",
    "#         point_cloud_xyz = point_cloud[:, 0:3] # num_point x 3 (x,y,z,reflectance) reflectance don't need\n",
    "#         point_cloud_xyz_hom = np.ones((point_cloud.shape[0], 4))\n",
    "#         point_cloud_xyz_hom[:, 0:3] = point_cloud[:, 0:3] # (point_cloud_xyz_hom has shape (num_points, 4))\n",
    "#         # the 4th column are all 1\n",
    "\n",
    "#         # project the points onto the image plane (homogeneous coords):\n",
    "#         img_points_hom = np.dot(P2, np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T))).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n",
    "#         # (U,V,_) = P2 * R0_rect * Tr_velo_to_cam * point_cloud_xyz_hom\n",
    "#         # normalize: (U,V,1)\n",
    "#         img_points = np.zeros((img_points_hom.shape[0], 2))\n",
    "#         img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n",
    "#         img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n",
    "\n",
    "#         # transform the points into (rectified) camera coordinates:\n",
    "#         point_cloud_xyz_camera_hom = np.dot(R0_rect, np.dot(Tr_velo_to_cam, point_cloud_xyz_hom.T)).T # (point_cloud_xyz_hom.T has shape (4, num_points))\n",
    "#         # normalize: (x,y,z,1)\n",
    "#         point_cloud_xyz_camera = np.zeros((point_cloud_xyz_camera_hom.shape[0], 3))\n",
    "#         point_cloud_xyz_camera[:, 0] = point_cloud_xyz_camera_hom[:, 0]/point_cloud_xyz_camera_hom[:, 3]\n",
    "#         point_cloud_xyz_camera[:, 1] = point_cloud_xyz_camera_hom[:, 1]/point_cloud_xyz_camera_hom[:, 3]\n",
    "#         point_cloud_xyz_camera[:, 2] = point_cloud_xyz_camera_hom[:, 2]/point_cloud_xyz_camera_hom[:, 3]\n",
    "\n",
    "#         point_cloud_camera = point_cloud\n",
    "#         point_cloud_camera[:, 0:3] = point_cloud_xyz_camera # reserve reflection\n",
    "\n",
    "#         ########################################################################\n",
    "#         # point_cloud               n x 4   original xyzr value before cali in velo coordinate\n",
    "#         # point_cloud_xyz           n x 3   xyz value before cali in velo coordinate\n",
    "#         # point_cloud_xyz_hom       n x 4   xyz1 in velo coordinate\n",
    "#         # point_cloud_xyz_camera    n x 4   xyz1 in camera coordinate\n",
    "#         # point_cloud_camera        n x 4   xyzr in camera coordinate\n",
    "#         # img_points_hom            n x 3   uv_\n",
    "#         # img_points                n x 2   UV\n",
    "#         ########################################################################\n",
    "\n",
    "\n",
    "#         row_mask = np.logical_and(\n",
    "#                         np.logical_and(img_points[:, 0] >= u_left,\n",
    "#                                        img_points[:, 0] <= u_right),\n",
    "#                         np.logical_and(img_points[:, 1] >= v_upper,\n",
    "#                                        img_points[:, 1] <= v_bottom))\n",
    "\n",
    "#         # filter out point are not in frustum area\n",
    "#         frustum_point_cloud_xyz = point_cloud_xyz[row_mask, :] # (needed only for visualization)\n",
    "#         frustum_point_cloud = point_cloud[row_mask, :]\n",
    "#         frustum_point_cloud_xyz_camera = point_cloud_xyz_camera[row_mask, :]\n",
    "#         frustum_point_cloud_camera = point_cloud_camera[row_mask, :]\n",
    "\n",
    "#         # randomly sample 512 points in the frustum point cloud:\n",
    "\n",
    "\n",
    "#         if frustum_point_cloud.shape[0] == 0:\n",
    "#              detection[7] = D_rough\n",
    "#              return torch.tensor(detection)\n",
    "#         # elif frustum_point_cloud.shape[0] < 512:\n",
    "#         #     row_idx = np.random.choice(frustum_point_cloud.shape[0], 512, replace=True)\n",
    "#         # else:\n",
    "#         #     row_idx = np.random.choice(frustum_point_cloud.shape[0], 512, replace=False)\n",
    "\n",
    "#         frustum_point_cloud_xyz_camera = frustum_point_cloud_xyz_camera[row_idx, :]\n",
    "#         ransac = linear_model.RANSACRegressor()\n",
    "#         ransac.fit(frustum_point_cloud_xyz_camera[:,1].reshape(-1,1),frustum_point_cloud_xyz_camera[:,0].reshape(-1,1))\n",
    "\n",
    "#         right_side_distance = ransac.predict([[frustum_point_cloud_xyz_camera[:,1].max()]])[0][0]\n",
    "#         left_side_distance = ransac.predict([[frustum_point_cloud_xyz_camera[:,1].min()]])[0][0]\n",
    "\n",
    "#         detection[7] = min(min(left_side_distance,right_side_distance),D_rough-2)\n",
    "#         print('image id:', img_id)\n",
    "#         print('Rough estimation %d, \\n ransac estimation: %d %d, \\n final estimation: %d' %(rough_D,left_side_distance,right_side_distance,detection[7]))\n",
    "#         return torch.tensor(detection)\n",
    "\n",
    "#     else:# might be a problem\n",
    "#         detection[7] = float('nan')\n",
    "#         return torch.tensor(detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
